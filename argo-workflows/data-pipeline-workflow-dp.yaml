apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: data-pipeline- # Argo will add a random suffix
spec:
  entrypoint: data-pipeline-dag
  arguments:
    parameters:
      - name: input-bucket
        value: "raw-data"
      - name: input-key # Path to the raw data file in the input-bucket
        value: "ams-data.csv" # Default value, can be overridden on submission
      - name: processed-bucket
        value: "processed-data"
      - name: reports-bucket
        value: "reports"
      - name: feature-bucket
        value: "feature-store"
      - name: processed-key-suffix
        value: "-processed.parquet"
      - name: bias-report-key-suffix
        value: "-bias-report.json"
      - name: train-key-suffix
        value: "-train.parquet"
      - name: test-key-suffix
        value: "-test.parquet"
      - name: val-key-suffix
        value: "-val.parquet"
      - name: sensitive-features # Comma-separated string
        value: "sensitive_attr" # Example: Adjust to data
      - name: target-column
        value: "target" # Example: Adjust to  data

  templates:
    # ----------------- VALIDATION STEP -----------------
    - name: validate-data
      inputs:
        parameters:
          - name: bucket
          - name: key
      container:
        image: step-validation:latest # Use locally built image
        imagePullPolicy: Never # Important for local images in Minikube's daemon
        command: [python, /app/validate.py]
        args: [
          "--bucket", "{{inputs.parameters.bucket}}",
          "--key", "{{inputs.parameters.key}}"
        ]
        env: # --- CORRECTED ENV BLOCK ---
          - name: S3_ENDPOINT_URL
            value: "http://minio-service.argo.svc.cluster.local:9000" # Adjust namespace if needed
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio-secrets # The k8s secret 
                key: rootUser
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootPassword
        # --- END ENV BLOCK ---

    # ----------------- PREPROCESSING STEP -----------------
    - name: preprocess-data
      inputs:
        parameters:
          - name: in_bucket
          - name: in_key
          - name: out_bucket
          - name: out_key
      container:
        image: step-preprocess:latest
        imagePullPolicy: Never
        command: [python, /app/preprocess.py]
        args: [
          "--in_bucket", "{{inputs.parameters.in_bucket}}",
          "--in_key", "{{inputs.parameters.in_key}}",
          "--out_bucket", "{{inputs.parameters.out_bucket}}",
          "--out_key", "{{inputs.parameters.out_key}}"
        ]
        env: # --- ENV BLOCK ---
          - name: S3_ENDPOINT_URL
            value: "http://minio-service.argo.svc.cluster.local:9000" # Adjust namespace if needed
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootUser
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootPassword
        # --- END ENV BLOCK ---

    # ----------------- BIAS CHECK STEP -----------------
    - name: check-bias
      inputs:
        parameters:
          - name: in_bucket
          - name: in_key
          - name: out_bucket # For report
          - name: report_key
          - name: sensitive_features
          - name: target_col
      container:
        image: step-check-bias:latest
        imagePullPolicy: Never
        command: [python, /app/check-bias-eo.py]
        args: [
          "--in_bucket", "{{inputs.parameters.in_bucket}}",
          "--in_key", "{{inputs.parameters.in_key}}",
          "--out_bucket", "{{inputs.parameters.out_bucket}}",
          "--report_key", "{{inputs.parameters.report_key}}",
          "--sensitive_features", "{{inputs.parameters.sensitive_features}}",
          "--target_col", "{{inputs.parameters.target_col}}"
        ]
        env: # ---  ENV BLOCK ---
          - name: S3_ENDPOINT_URL
            value: "http://minio-service.argo.svc.cluster.local:9000" # Adjust namespace if needed
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootUser
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootPassword
        # --- END ENV BLOCK ---

    # ----------------- DATA SPLITTING STEP -----------------
    - name: split-data
      inputs:
        parameters:
          - name: in_bucket
          - name: in_key
          - name: out_bucket # For splits
          - name: train_key
          - name: test_key
          - name: val_key
          - name: target_col
      container:
        image: step-split-data:latest
        imagePullPolicy: Never
        command: [python, /app/split_data.py]
        args: [
          "--in_bucket", "{{inputs.parameters.in_bucket}}",
          "--in_key", "{{inputs.parameters.in_key}}",
          "--out_bucket", "{{inputs.parameters.out_bucket}}",
          "--train_key", "{{inputs.parameters.train_key}}",
          "--test_key", "{{inputs.parameters.test_key}}",
          "--val_key", "{{inputs.parameters.val_key}}",
          "--target_col", "{{inputs.parameters.target_col}}"
          # Add --test_size, --val_size, --random_state if needed
        ]
        env: # ---  ENV BLOCK ---
          - name: S3_ENDPOINT_URL
            value: "http://minio-service.argo.svc.cluster.local:9000" # Adjust namespace if needed
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootUser
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio-secrets
                key: rootPassword
        # --- END  ENV BLOCK ---

    # ----------------- PIPELINE DAG DEFINITION -----------------
    - name: data-pipeline-dag
      dag:
        tasks:
          - name: validate
            template: validate-data
            arguments:
              parameters:
              - name: bucket
                value: "{{workflow.parameters.input-bucket}}"
              - name: key
                value: "{{workflow.parameters.input-key}}"

          - name: preprocess
            template: preprocess-data
            dependencies: [validate] # Runs after validation completes
            arguments:
              parameters:
              - name: in_bucket
                value: "{{workflow.parameters.input-bucket}}"
              - name: in_key # Use the original input key for preprocessing
                value: "{{workflow.parameters.input-key}}"
              - name: out_bucket
                value: "{{workflow.parameters.processed-bucket}}"
              - name: out_key # Construct the output key dynamically
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.processed-key-suffix}}"

          - name: bias-check
            template: check-bias
            dependencies: [preprocess] # Runs after preprocessing completes
            arguments:
              parameters:
              - name: in_bucket # Input is the processed data
                value: "{{workflow.parameters.processed-bucket}}"
              - name: in_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.processed-key-suffix}}"
              - name: out_bucket # Output is the report bucket
                value: "{{workflow.parameters.reports-bucket}}"
              - name: report_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.bias-report-key-suffix}}"
              - name: sensitive_features
                value: "{{workflow.parameters.sensitive-features}}"
              - name: target_col
                value: "{{workflow.parameters.target-column}}"

          - name: split
            template: split-data
            dependencies: [bias-check] # Runs after bias check completes
            arguments:
              parameters:
              - name: in_bucket # Input is the processed data
                value: "{{workflow.parameters.processed-bucket}}"
              - name: in_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.processed-key-suffix}}"
              - name: out_bucket # Output is the feature store bucket
                value: "{{workflow.parameters.feature-bucket}}"
              - name: train_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.train-key-suffix}}"
              - name: test_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.test-key-suffix}}"
              - name: val_key
                value: "{{workflow.parameters.input-key}}{{workflow.parameters.val-key-suffix}}"
              - name: target_col
                value: "{{workflow.parameters.target-column}}"